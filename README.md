# spider文件夹

## try

### two

​		get_urls.py: 保存廖雪峰网站，save成pdf

​		words_try.py:  持久化到mysql

### one

​		key_word_relation.py: 添加cookie，设置key_words，爬取微博、及其用户关系网。

## baidu_picture文件夹

输入关键词，爬取百度图片网站上相关图片

​		

# scrapyWenDang 文件夹

## CreatePool

​		scrapy 爬虫框架，爬取 'xicidaili' 网站，建立ip代理池，并定时测试Ip 代理池中的有效ip



## RewriteXiaohua



​		重写xiaohua爬虫，爬取xiaohua.com网站。自定义save Image method 或者启用scrapy内部图片pipelines (imagePiplines)

# CrawlWenDang 文件夹



## createIpPool

​		使用requests爬取代理网站，建立ip池



## **Novel**

​		空

## **LiaoxueFeng**.py

​		爬取廖雪峰网站，保存为pdf格式

## **SFAnimated**.py

​		爬取某个漫画网站某一个漫画的图片([萌师在上](https://github.com/ficherfisher/picture/tree/picture/crawlWengDang/萌师在上) 文件夹为爬取结果)

## **weiBo**.py

​		爬取微博某一话题，保存文本、图片等信息save为pdf([WeiBo](https://github.com/ficherfisher/picture/tree/picture/crawlWengDang/WeiBo) 文件夹为结果)



## weiboCookie.py

​		建立微博cookie池，自动登录微博。([WeiboCookie](https://github.com/ficherfisher/picture/tree/picture/crawlWengDang/WeiboCookie) 文件夹为结果)

## **Xiaohua.py**

​		xiaohua爬虫，爬取xiaohua.com网站。([xiaohua Data](https://github.com/ficherfisher/picture/tree/picture/crawlWengDang/xiaohua Data) 文件夹为结果)

## update.py

​		爬取高清壁纸网站上的图片
